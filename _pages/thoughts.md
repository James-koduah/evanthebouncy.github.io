---
title:
layout: default
permalink: /thoughts/
published: true
---

### Thoughts

Here are scattered thoughts that I think are cool.
They range from ideas, quotes from others, observations, a dream I had, opinions. Think of these as passages found in fortune cookies. Treat with open minds and cautions.

In the near future, a LLM will be able to summarize this body of text, and you'd be able to interact with it through a natural interface, which can retrieve the right quotes from this text document. 

Written in no particular order, but recent thoughts tends to be on top.

----

[x] you want to make sure collaboration is in agreement on all levels: why -- why are we doing what we're doing? what -- what we're doing in order to meet the why? how -- the actual details. the agreement needs to be done in this order. make sure the team is in agreement on all levels, in this order. otherwise you can't move forward.

[x] accomplishment is cool. getting things done are cool. "I don't care" is not cool at all, as it doesn't actually make anything.

[x] there is a trade-off of automation and flexibility. typically you choose one or the other, but if you do it right you can do both.

[x] good research address a bottleneck of a community that is current. you want to align this with your own interests.

[x] validity = complexity in the right direction. you want to work on valid problems, not arbitrarily complex problems for no good reason.

[x] I think humans place a disporportional amount of importance on energetic systems, as we are intrinstically energy seeking creatures. this reflects in fictions and politics, where we imagine adversaries and violence as our biggest threats -- someone out there to "get us". but the world is energy starved, and most of threats are the lack of energy, the boring dispassionate threat of coldness and starvation.

[x] The Dialectics of Sketching: to _design_ is to plan for the making of something new.

[x] josh: building collaborations with people you can learn from — not just do good work with, but learn from their approaches to research — is very good.

[x] neural models -- good at bullshit almost right answers better than random. programs -- work tirelessly over large input sizes but brittle. reasoning (symbolic or probablistic) -- exhaustive over small domains and accurate, but unscalable. humans -- can do all 3 of the above if given enough time, but is expensive. we need to combine these things together in a truly collaborative system.

[x] I need to work on having a tighter research statement sooner. my iteration speed on projects is lacking, because I explore too much and don't clamp down soon enough. focus on having a clear statement sooner. having people critique my work before all the details are fleshed out is helpful.

[x] Robert Hawkins: as soon as you have a dictionary it becomes out of date. meanings change over time. I wonder what this has to do with how crappy programming language semantics, and DSL are?

[x] tagging an image, providing a label, providing a caption. these are the bricks of the dataset pyramid. these are very regular in "shape" and easy to "stack". however, more interesting interactions, like the way human teach each other, do not have regular "shapes". they are unique experiences and machines don't know how to aggregate or "stack" them. we need to stack these weird but powerful bricks, build systems that can do that.

[x] pyramids are cool. they're cool because it is a physical accumulator. you can explain what makes a pyramid to anyone, an algorithm about moving rocks and placing them, and _everyone_ can execute this algorithm. this shared problem, everyone contributing to it. a good research statement should strive to be the same. A research direction shall be addative. A good way of being addative is by being measurable (think benchmark and dataset) is good, fields that can be measurable improve faster (sam told me this).

[x] programming system should be viewed as a knowledge accumulator. more people interact, more knowledge accumulated within. much like how a person accumulates knowledge. stackoverflow and github are kinda like this, but not quite there. copilot is even further not there.

[x] wisdom of crowd isn't an aggregation over opinions, that is ungrounded and flaccid. wisdom of crowd is sharing a problem that everyone can agree on is important to solve, that a good solution is hard to find, yet apparent once found (think NP). everyone attempts to solve it in their own way. blast out the search space. more people, more likely to find a good solution. that is true wisdom of the crowd. once a solution is found, everyone benefits.

[x] Gerald Sussman said about code/programs along the line of: I can give up general correctness for ease of use right here and now. It's hard to prove general things, so you make a specialized case. But that is brittle.

[x] LLM will suck at playing 20 questions, as it forces a consistent internal model. It will suck even more at playing [situation puzzles](https://en.wikipedia.org/wiki/Situation_puzzle). This will be a very good turing test. If an LLM can play a situation puzzle with a participant, I'd be very surprised and happy. The hilarious part is for a LLM it is very easy to play the role of the oracle in these games, so you can set this up in a self-play way.

[x] a good way to talk about auto-complete is to have the notion of a past set and a future set. the past set is what you've already given the system, and the system's job is to predict the future set. the past set and future set together is the completed thing.

[x] edge is a cool word. in research, what is your edge over other people? what are something only you can do?

[x] Daniel fried said for auto-complete like code, short time-horizon is good, i.e. predict the next thing instead of predicting everything at once. this is intuitive.

[x] abduction -- spinning up a DSL "on the fly" for a particular task is a very important problem. it seems human does this, and it solves the "no DSL closure" issue.

[x] learning via associations is essentially learning with redundancy. two things `A, B`, are given to you, you make some kind of relationship between them (think conditional probability `P(B|A))`. this relationship can be used to recover (partially) one in the absence of another, i.e. redundancy.

[x] beilei on branding: say you're person A. how would someone else, B, explain your work to C? this explanation is basically your brand.

[x] someone in design probably: "whenever someone is using a tool, they're having a conversation with its creator."

[x] josh : "are you evaluating it or you're just guessing?" "is this effect big or small?"

[x] dataset can be same or different. analysis can be same or different. analysis = same dataset, same analysis; replication = different dataset, same analysis; robust is same dataset, different analysis; generalizable is different dataset, different analysis.

[x] leo quoting tao: "once you have the right solution, making it work on something impressive does not scale linearly with efforts." thus, with the right idea, you can push it to something more impressive.

[x] kevin : "interpretable equations have 'units', i.e. F = Gm1m2/rr, whereas the 'equations' implemented in a neural network have no 'units'.

[x] robert : "convention exists in a community, but everyone's experience is personal"

[x] asolar : people re-use primitives in a language because they can be manipulated and understood easily. however, library is essentially a new language and require a lot of effort to learn, thus, people re-implement. loops and if-then-else are pretty common across languages, but the more specific constructs are obscure.

[x] j.carrol on 'naming and cescribing in social communications': 1) the distiction between "name" and "description" is not clear at all, it is a gradient. 2) people reach a "name" through coordination and negotiation, in absence of observation of these processes, a name is unintelligible. 3) the shortening from description to name depends on situation, sometimes it shortens slowly when the set of distractors is difficult, and quickly if only few distractors are present. self note: compare this to programs, where "name" is function-name, and "description" is in a sense a function's body.

[x] josh : "the goal of the abstract is to get the right people interested in reading the paper, but not to overpromise. who should read this paper and why?"

[x] robert : you can give data to psych people in two forms. wide-form, where each user is 1 row cllapsed in a style of json dictionary. long-form, where each unit of analysis is 1 row expanded out (R likes this form more).

[x] marta : when you get data, first thing to do is to do _descriptive statistics_ such as number of participants, average length of descriptions, etc.

[x] josh : "the most striking thing about our data is ...". "this quantitative result can _carry_ the paper alone.

[x] josh : have a hypothesis before going out to collect data, think deeply before going yolo.

[x] in collaboration it is helpful to explain straightforwardly what difficulty _you_ currently have, so everyone is on the same page. also asks other to explain theirs. share these difficulties, share these "cost functions" and optimize together.

[x] alex bishara : if you want to amplify a direction of research, it should be done in a manner that is irrelevant if people continue to work with you or not. ultimately they should be able to go in that direction independently.

[x] robert hawkins : the other approaches are not competition, but a "foil" which we can use to contextualize our own works.

[x] josh : what is the question? how should we answer them? in another words, can you show me a plot, and how the plot turned up will answer the question one way or another?

[x] "hold strong opinions loosely"